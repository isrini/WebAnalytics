{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "\n",
    "The main goal of this project is to build a web crawler and scrape the text related to American movies, which were released between 2000 and 2016 years. Along with the text, we will also crawl the Movies posters. The major deliverables of this project are:\n",
    "\n",
    "1. Text (or plot) of American movies, which were released between the years 2000 and 2016\n",
    "2. Movies release posters\n",
    "\n",
    "All the data will be obtained from Wikipedia.\n",
    "\n",
    "\n",
    "## Design\n",
    "\n",
    "Wikipedia maintains list of movies, released in each year. The list of American movies released in each year are present at https://en.wikipedia.org/wiki/List_of_American_films_of_XXXX, where XXXX is the year. For each year between 2000 and 2016 (XXXX = 2000 to 2016), we have to recurrsively visit each year's URL to obtain the movies URLs, along with movie details such as cast, director, genre etc. Once the URLs related to all the movies are obtained, the web crawler will visit the movie URLs to scrape the plot of the movie. \n",
    "\n",
    "The following flowchart will provide an overview of the web crawling process:\n",
    "\n",
    "<img src=\"crawler_logic.png\">\n",
    "\n",
    " **Figure-1: Web crawling process**\n",
    "\n",
    "The whole web crawling process is divided into 2 steps. In STEP-1, we will visit Wikipedia to obtain a list of all the URLs related to the American movies which were released between the years 2000-2016. In STEP-2, we will visit each of the URLs obtained in STEP-1, to download movie text and release poster. There must be some delay of 2 to 3 seconds between successive requests to Wikipedia website.\n",
    "\n",
    "The output of STEP-1 is a comma separated file (Movie_Details.csv), with the following details: \n",
    "\n",
    "**Movie** - Movie Name\n",
    "\n",
    "**URL** - Wikipedia web page for the movie\n",
    "\n",
    "**Year** - Year of release\n",
    "\n",
    "**Director** - Director of the movie\n",
    "\n",
    "**Cast** - Cast of the movie\n",
    "\n",
    "**Genre** - Movie's genre\n",
    "\n",
    "**Movie_ID** - Unique key to distinguish each movie\n",
    "\n",
    "The output of STEP-2 will be a set of text files, each file named using the Movie_ID, and a set of image files, each of the image files will also be named using the Movie_ID. Naming the files using the Movie_ID will help us to refer uniquely identify the text and image files related to a movie uniquely and easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required packages\n",
    "Let us import all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import time\n",
    "import pickle #To save the objects that were created using webscraping\n",
    "import pprint\n",
    "from lxml import html\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "#import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### STEP-1 Implementation\n",
    "\n",
    "In STEP-1 of the web crawling process, we will obtain the list of movies, along with the URLs of the movies, for all the movies which were released between the years 2000 and 2016. \n",
    "\n",
    "But the challenge was, the format of the HTML file. Wikipedia used one format for the movies released between 2000 and 2013, and another format for the movies released in 2014-16. So we will sub-divide the STEP-1 process further to scrape the list of movies between 2000 and 2013 in phase 1 and 2015-16 in phase2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##PHASE-1: Get the movies and URLs for the years 2000-2013\n",
    "#Define the lists to hold the details of the movies\n",
    "URL = list()\n",
    "Movie_Name = list()\n",
    "Director = list()\n",
    "Cast = list()\n",
    "Genre = list()\n",
    "year = list()\n",
    "\n",
    "#Create a beautiful soup object\n",
    "bs = BeautifulSoup(html)\n",
    "\n",
    "#Iterate over the years 2000 to 2014.\n",
    "for y in list(range(2000,2014)):\n",
    "    \n",
    "    #Prepare the URL String and open the URL\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_American_films_of_\"+str(y)\n",
    "    html = urlopen(url)\n",
    "    \n",
    "    #Mandatory wait of 3 seconds\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #Get the web page as HTML document\n",
    "    bs = BeautifulSoup(html)\n",
    "    \n",
    "    #Parse and get the required data\n",
    "    for table in bs.find_all('table', {\"class\":\"wikitable\"}):\n",
    "        for row in table.find_all('tr'):\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) > 4:\n",
    "                Movie_Name.append(columns[0].get_text())\n",
    "                Director.append(columns[1].get_text())\n",
    "                Cast.append(columns[2].get_text())\n",
    "                Genre.append(columns[3].get_text())\n",
    "                year.append(y)\n",
    "                \n",
    "                #Handle exceptions, so that the process continues\n",
    "                try:\n",
    "                    a = columns[0].find('a',href=True)['href']\n",
    "                    URL.append(\"https://en.wikipedia.org\"+a)\n",
    "                except:\n",
    "                    URL.append(\"NA\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##PHASE-2: Get the movies details for the years 2014 to 2016\n",
    "#Declare the lists\n",
    "URL1 = list()\n",
    "Movie_Name1 = list()\n",
    "Director1 = list()\n",
    "Cast1 = list()\n",
    "Genre1 = list()\n",
    "year1 = list()\n",
    "\n",
    "#For the years between 2014 and 2016\n",
    "for y in range(2014,2017):          \n",
    "    \n",
    "    #Prepare wiki URL\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_American_films_of_\"+str(y)\n",
    "    \n",
    "    #Exception handling to ignore the failures and continue processing\n",
    "    try: \n",
    "        html = urlopen(url)\n",
    "    except:\n",
    "        print(\"problem with the following URL...continuining...:\")\n",
    "        print(url)\n",
    "        continue\n",
    "    #Sleep for 3 secs    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    #Declare beautiful soup object\n",
    "    bs = BeautifulSoup(html)\n",
    "    for table in bs.find_all('table', {\"class\":\"wikitable\"}):\n",
    "        for row in table.find_all('tr'):\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) > 3: #To make sure that we are accessing the movies tables only\n",
    "                if len(columns) == 6:\n",
    "                    #print(columns[0].get_text())\n",
    "                    Movie_Name1.append(columns[0].get_text())\n",
    "                    Director1.append(columns[1].get_text())\n",
    "                    Cast1.append(columns[2].get_text())\n",
    "                    Genre1.append(columns[3].get_text())\n",
    "                    year1.append(y)\n",
    "                    try:\n",
    "                        a=columns[0].find('a',href=True)['href']\n",
    "                        URL1.append(\"https://en.wikipedia.org\"+a)\n",
    "                    except:\n",
    "                        URL1.append(\"NA\")\n",
    "                        continue\n",
    "                \n",
    "                if len(columns) == 7:\n",
    "                    #print(columns[1].get_text())\n",
    "                    Movie_Name1.append(columns[1].get_text())\n",
    "                    Director1.append(columns[2].get_text())\n",
    "                    Cast1.append(columns[3].get_text())\n",
    "                    Genre1.append(columns[4].get_text())\n",
    "                    year1.append(y)\n",
    "                    \n",
    "                    try:\n",
    "                        a=columns[1].find('a',href=True)['href']\n",
    "                        URL1.append(\"https://en.wikipedia.org\"+a)\n",
    "                    except:\n",
    "                        URL1.append(\"NA\")\n",
    "                        continue\n",
    "\n",
    "                if len(columns) > 7:\n",
    "                    #print(\"col len:{}\".format(len(columns)))\n",
    "                    #print(columns[2].get_text())\n",
    "                    Movie_Name1.append(columns[2].get_text())\n",
    "                    Director1.append(columns[3].get_text())\n",
    "                    Cast1.append(columns[4].get_text())\n",
    "                    Genre1.append(columns[5].get_text())\n",
    "                    year1.append(y)                    \n",
    "                    try:\n",
    "                        a=columns[2].find('a',href=True)['href']\n",
    "                        URL1.append(\"https://en.wikipedia.org\"+a)\n",
    "                    except:\n",
    "                        URL1.append(\"NA\")\n",
    "                        continue\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results\n",
    "We will save the movies details as a CSV file named Movie_Details.csv. This helps us to avoid running step-1 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a data frame:\n",
    "df = pd.DataFrame(list(zip(Movie_Name+Movie_Name1,URL+URL1,year+year1,Director+Director1,Cast+Cast1,Genre+Genre1)),\\\n",
    "                  columns=[\"Movie\",\"URL\",\"Year\",\"Director\",\"Cast\",\"Genre\"])\n",
    "\n",
    "#Remove the rows which do not have URL information\n",
    "df = df[df[\"URL\"] != \"NA\"]\n",
    "\n",
    "df[\"Movie_ID\"] = df.index + 1\n",
    "\n",
    "#Write the file\n",
    "df.to_csv(\"Movies_Details.csv\",encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the saved file to a data frame.\n",
    "We will read back the saved file to a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>URL</th>\n",
       "      <th>Year</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Movie_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102 Dalmatians</td>\n",
       "      <td>https://en.wikipedia.org/wiki/102_Dalmatians</td>\n",
       "      <td>2000</td>\n",
       "      <td>Kevin Lima</td>\n",
       "      <td>Glenn Close, Gérard Depardieu, Alice Evans</td>\n",
       "      <td>Comedy, family</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28 Days</td>\n",
       "      <td>https://en.wikipedia.org/wiki/28_Days_(film)</td>\n",
       "      <td>2000</td>\n",
       "      <td>Betty Thomas</td>\n",
       "      <td>Sandra Bullock, Viggo Mortensen</td>\n",
       "      <td>Drama</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 Strikes</td>\n",
       "      <td>https://en.wikipedia.org/wiki/3_Strikes_(film)</td>\n",
       "      <td>2000</td>\n",
       "      <td>DJ Pooh</td>\n",
       "      <td>Brian Hooks, N'Bushe Wright</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 6th Day</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_6th_Day</td>\n",
       "      <td>2000</td>\n",
       "      <td>Roger Spottiswoode</td>\n",
       "      <td>Arnold Schwarzenegger, Robert Duvall</td>\n",
       "      <td>Science fiction</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Across the Line</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Across_the_Line_...</td>\n",
       "      <td>2000</td>\n",
       "      <td>Martin Spottl</td>\n",
       "      <td>Brad Johnson, Adrienne Barbeau, Brian Bloom</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Movie                                                URL  Year  \\\n",
       "0    102 Dalmatians       https://en.wikipedia.org/wiki/102_Dalmatians  2000   \n",
       "1           28 Days       https://en.wikipedia.org/wiki/28_Days_(film)  2000   \n",
       "2         3 Strikes     https://en.wikipedia.org/wiki/3_Strikes_(film)  2000   \n",
       "3       The 6th Day          https://en.wikipedia.org/wiki/The_6th_Day  2000   \n",
       "4   Across the Line  https://en.wikipedia.org/wiki/Across_the_Line_...  2000   \n",
       "\n",
       "             Director                                         Cast  \\\n",
       "0          Kevin Lima   Glenn Close, Gérard Depardieu, Alice Evans   \n",
       "1        Betty Thomas              Sandra Bullock, Viggo Mortensen   \n",
       "2             DJ Pooh                  Brian Hooks, N'Bushe Wright   \n",
       "3  Roger Spottiswoode         Arnold Schwarzenegger, Robert Duvall   \n",
       "4       Martin Spottl  Brad Johnson, Adrienne Barbeau, Brian Bloom   \n",
       "\n",
       "             Genre  Movie_ID  \n",
       "0   Comedy, family         1  \n",
       "1            Drama         2  \n",
       "2           Comedy         3  \n",
       "3  Science fiction         4  \n",
       "4         Thriller         5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>URL</th>\n",
       "      <th>Year</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Movie_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>Inferno</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Inferno_(2016_film)</td>\n",
       "      <td>2016</td>\n",
       "      <td>Ron Howard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>Friend Request</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Friend_Request</td>\n",
       "      <td>2016</td>\n",
       "      <td>Simon Verhoeven</td>\n",
       "      <td>Alycia Debnam-Carey</td>\n",
       "      <td>Horror</td>\n",
       "      <td>4042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>Rogue One: A Star Wars Story (film)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rogue_One</td>\n",
       "      <td>2016</td>\n",
       "      <td>Felicity Jones</td>\n",
       "      <td>Diego Luna</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>4043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>The Founder</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Founder_(film)</td>\n",
       "      <td>2016</td>\n",
       "      <td>John Lee Hancock</td>\n",
       "      <td>Michael Keaton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>Rings</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rings_(2016_film)</td>\n",
       "      <td>2016</td>\n",
       "      <td>F. Javier Gutiérrez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Movie  \\\n",
       "4040                              Inferno   \n",
       "4041                       Friend Request   \n",
       "4042  Rogue One: A Star Wars Story (film)   \n",
       "4043                          The Founder   \n",
       "4044                                Rings   \n",
       "\n",
       "                                                    URL  Year  \\\n",
       "4040  https://en.wikipedia.org/wiki/Inferno_(2016_film)  2016   \n",
       "4041       https://en.wikipedia.org/wiki/Friend_Request  2016   \n",
       "4042            https://en.wikipedia.org/wiki/Rogue_One  2016   \n",
       "4043   https://en.wikipedia.org/wiki/The_Founder_(film)  2016   \n",
       "4044    https://en.wikipedia.org/wiki/Rings_(2016_film)  2016   \n",
       "\n",
       "                 Director                 Cast   Genre  Movie_ID  \n",
       "4040           Ron Howard                  NaN     NaN      4041  \n",
       "4041      Simon Verhoeven  Alycia Debnam-Carey  Horror      4042  \n",
       "4042       Felicity Jones           Diego Luna  Sci-Fi      4043  \n",
       "4043     John Lee Hancock       Michael Keaton     NaN      4044  \n",
       "4044  F. Javier Gutiérrez                  NaN     NaN      4045  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4045, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = pd.read_csv(\"Movie_Details.csv\")\n",
    "\n",
    "display(URL.head())\n",
    "display(URL.tail())\n",
    "\n",
    "URL.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are 4045 movie URLs that have to be scraped from Wikipedia. Let us do this as batches. Our goal is to scrape the image of the movie (if exists), along with the plot and initial introduction texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP-2 Implementation\n",
    "\n",
    "In STEP-2 we will use the file Movie_Details.csv, which has the list of all movie URLs, along with some other details. Each URL of the movie will be crawled to extract the movies text and the images. In STEP-2, we will build a set of functions to perform the web crawling. These functions are explained below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "We will code the following functions to obtain the plot information of the movie, along with the release poster image of the movie.\n",
    "\n",
    "* **Open_URL(url)** Gets the HTML content, prepares Beautiful Soup object and returns the Beautiful Soup object. The _url_ parameter represents the complete URL of the webpage.\n",
    "\n",
    "* **Get_Plot(bs)** Takes a beautiful soup object as input. It extracts the introductory text, and the text in the section _plot_\n",
    "If plot section is NOT present, then it return a negative code. The function returns the extracted text (in the _plot_ section and the initial paragraph). If an error occurs, then a negative code is returned.\n",
    "    Return code = -1: If an error has occurred while getting the paragraphs from bs object\n",
    "    Return code = -2: If there is NO _Plot_ section in the document\n",
    "    Return code = -3: If an error occurred while extracting the first paragrapg in HTML doc\n",
    "\n",
    "* **Get_All_Text(bs)** This function is called only when **Get_Plot(bs)** returns a -2. This function will take a beautiful object as input and gives all the text (present in < p > tags) as output. It will return -1, if any error occurs.\n",
    "\n",
    "* **Save_Text_File(text,text_file_name)** It will save the text string (_text_) as a text file (with the name contained in *text_file_name*). The file is saved into the _data_ directory. Returns 0, if sucessfully saved. Returns -1, when the change directory command fails (while changing to _data_ sub-directory), and -2 when the change to parent directory from _data_ fails.\n",
    "\n",
    "* **Get_And_Save_Image(bs,image_file_name)** It will get the movies poster (image file) and saves the image in the _images_ directory. It will take beautiful soup object as input and extracts the image URL. The image URL will be downloaded and saved to _images_ directory with the file name as the value present in *image_file_name*. Returns 0, if successfully downloaded and saved. Returns -1 if image is not found, and -2 if the an error occurs when saving the image.\n",
    "\n",
    "* **Write_Error(url,msg,file)** Will write a error/warning message (contained in the parameter _msg_), while parsing the URL (present in the _url_ parameter). The _file_ parameter contains the name of the error file.  \n",
    "\n",
    "The source code of these functions is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Open_URL(url):\n",
    "    '''\n",
    "    Gets the HTML content, prepares Beautiful Soup object and \n",
    "    returns the  Beautiful Soup object. \n",
    "    The url parameter represents the complete URL of the webpage.\n",
    "    '''\n",
    "    try:\n",
    "         html = urlopen(url)\n",
    "    except:\n",
    "        return -1\n",
    "    try:\n",
    "        #bs = BeautifulSoup(html).encode(\"ascii\")\n",
    "        bs = BeautifulSoup(html)\n",
    "        \n",
    "        return bs\n",
    "    except:\n",
    "        return -2       \n",
    "\n",
    "def Get_Plot(bs):  \n",
    "    \"\"\"\n",
    "    Takes a beautiful soup object as input.\n",
    "    Extracts the introductory text, and the text in the section plot\n",
    "    If plot section is NOT present, just get all the available text in the webpage\n",
    "    Returns the extracted text (if NO error, else returns a negative error code).\n",
    "    -1: If an error has occurred while getting the paragraphs from bs object\n",
    "    -2: If an error occurred while extracting the plot text\n",
    "    -3: If an error occurred while extracting the first paragrapg in HTML doc\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = bs.find(\"p\")\n",
    "        initial_paragraph = p.getText()\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    # collect plot in this list\n",
    "    plot = []\n",
    "    \n",
    "    # find the node with id of \"Plot\"\n",
    "    try:\n",
    "        mark = bs.find(id=\"Plot\")\n",
    "        # walk through the siblings of the parent (H2) node \n",
    "        # until we reach the next H2 node\n",
    "        for elt in mark.parent.nextSiblingGenerator():\n",
    "            if elt.name == \"h2\":\n",
    "                break\n",
    "            if hasattr(elt, \"text\"):\n",
    "                plot.append(elt.text)\n",
    "    except:\n",
    "         return -2\n",
    "    \n",
    "    try:\n",
    "        plot=\"\".join(plot)\n",
    "        text = initial_paragraph + plot\n",
    "        return text\n",
    "    except:\n",
    "        return -3    \n",
    "    \n",
    "\n",
    "    \n",
    "def Get_All_Text(bs):\n",
    "    try:\n",
    "        p = bs.find_all(\"p\")\n",
    "        l = list()\n",
    "        for i in p:\n",
    "            l.append(i.getText())\n",
    "        return \" \".join(l)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def Save_Text_File(text,text_file_name):\n",
    "    try:\n",
    "        os.chdir(\"./data\")\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    with open(text_file_name, 'w',encoding='utf-8') as f:\n",
    "         f.write(text)\n",
    "    \n",
    "    try:\n",
    "        os.chdir(\"..\")\n",
    "        return 0\n",
    "    except:\n",
    "        return -2\n",
    "\n",
    "def Get_And_Save_Image(bs,image_file_name):    \n",
    "    try: \n",
    "        img=bs.findAll(\"img\",{\"class\":\"thumbborder\"})\n",
    "        img_URL=\"https:\"+img[0]['src']\n",
    "    except:\n",
    "        return -1\n",
    "     \n",
    "    try:\n",
    "        os.chdir(\"./images\")\n",
    "        ignore=urllib.request.urlretrieve(img_URL,image_file_name)\n",
    "        os.chdir(\"..\")\n",
    "        return 0\n",
    "    except:\n",
    "        return -2\n",
    "\n",
    "def Write_Error(url,msg,file):\n",
    "    with open(file,'a') as f:\n",
    "        f.write(\"\\n\"+msg)\n",
    "        f.write(\"\\n\"+url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning the crawling process\n",
    "\n",
    "The following code block will crawl the movies text from Wikipedia. This code ran for approximately 7 hours. So do NOT execute this code, unless you really want to start the download process. The output of this code is a series of text files and image files. The text files are saved to the _data_ directory and images to the _image_ directory. You can download these files directly from the location: https://goo.gl/RDhVtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning the files download...\n",
      "Processed 100 URLs\n",
      "Elapsed time to process 100 URLs:446.38204622268677 secs\n",
      "Processed 200 URLs\n",
      "Elapsed time to process 100 URLs:433.5029966831207 secs\n",
      "Processed 300 URLs\n",
      "Elapsed time to process 100 URLs:461.3205850124359 secs\n",
      "Processed 400 URLs\n",
      "Elapsed time to process 100 URLs:458.53809428215027 secs\n",
      "Processed 500 URLs\n",
      "Elapsed time to process 100 URLs:476.4255542755127 secs\n",
      "Processed 600 URLs\n",
      "Elapsed time to process 100 URLs:473.6112816333771 secs\n",
      "Processed 700 URLs\n",
      "Elapsed time to process 100 URLs:458.1378722190857 secs\n",
      "Processed 800 URLs\n",
      "Elapsed time to process 100 URLs:434.91878509521484 secs\n",
      "Processed 900 URLs\n",
      "Elapsed time to process 100 URLs:569.373973608017 secs\n",
      "Processed 1000 URLs\n",
      "Elapsed time to process 100 URLs:533.2878279685974 secs\n",
      "Processed 1100 URLs\n",
      "Elapsed time to process 100 URLs:493.94636368751526 secs\n",
      "Processed 1200 URLs\n",
      "Elapsed time to process 100 URLs:560.7728536128998 secs\n",
      "Processed 1300 URLs\n",
      "Elapsed time to process 100 URLs:519.7430667877197 secs\n",
      "Processed 1400 URLs\n",
      "Elapsed time to process 100 URLs:661.1075065135956 secs\n",
      "Processed 1500 URLs\n",
      "Elapsed time to process 100 URLs:468.4266812801361 secs\n",
      "Processed 1600 URLs\n",
      "Elapsed time to process 100 URLs:482.55657744407654 secs\n",
      "Processed 1700 URLs\n",
      "Elapsed time to process 100 URLs:468.59105467796326 secs\n",
      "Processed 1800 URLs\n",
      "Elapsed time to process 100 URLs:448.06637740135193 secs\n",
      "Processed 1900 URLs\n",
      "Elapsed time to process 100 URLs:454.26227259635925 secs\n",
      "Processed 2000 URLs\n",
      "Elapsed time to process 100 URLs:437.51902770996094 secs\n",
      "Processed 2100 URLs\n",
      "Elapsed time to process 100 URLs:437.7170066833496 secs\n",
      "Processed 2200 URLs\n",
      "Elapsed time to process 100 URLs:432.09807682037354 secs\n",
      "Processed 2300 URLs\n",
      "Elapsed time to process 100 URLs:429.547082901001 secs\n",
      "Processed 2400 URLs\n",
      "Elapsed time to process 100 URLs:431.0166103839874 secs\n",
      "Processed 2500 URLs\n",
      "Elapsed time to process 100 URLs:426.8530662059784 secs\n",
      "Processed 2600 URLs\n",
      "Elapsed time to process 100 URLs:434.2797124385834 secs\n",
      "Processed 2700 URLs\n",
      "Elapsed time to process 100 URLs:432.32950735092163 secs\n",
      "Processed 2800 URLs\n",
      "Elapsed time to process 100 URLs:462.65187335014343 secs\n",
      "Processed 2900 URLs\n",
      "Elapsed time to process 100 URLs:475.1674907207489 secs\n",
      "Processed 3000 URLs\n",
      "Elapsed time to process 100 URLs:446.28495264053345 secs\n",
      "Processed 3100 URLs\n",
      "Elapsed time to process 100 URLs:486.55276560783386 secs\n",
      "Processed 3200 URLs\n",
      "Elapsed time to process 100 URLs:478.56283259391785 secs\n",
      "Processed 3300 URLs\n",
      "Elapsed time to process 100 URLs:509.5021858215332 secs\n",
      "Processed 3400 URLs\n",
      "Elapsed time to process 100 URLs:468.18198013305664 secs\n",
      "Processed 3500 URLs\n",
      "Elapsed time to process 100 URLs:441.2250940799713 secs\n",
      "Processed 3600 URLs\n",
      "Elapsed time to process 100 URLs:445.52249574661255 secs\n",
      "Processed 3700 URLs\n",
      "Elapsed time to process 100 URLs:511.63219952583313 secs\n"
     ]
    }
   ],
   "source": [
    "tracker = 0\n",
    "term=1\n",
    "start = time.time() # Get start time\n",
    "print(\"Beginning the files download...\")\n",
    "#k = 1\n",
    "for movie, url, year,Movie_ID in zip(list(URL[\"Movie\"]),list(URL[\"URL\"]),list(URL[\"Year\"]),list(URL[\"Movie_ID\"])):\n",
    "    #if k < 30:\n",
    "    #    k = k+1\n",
    "    #    continue\n",
    "    #print(\"{},{},{}\".format(movie,url,year))\n",
    "    #Open the URL\n",
    "    \n",
    "    bs=Open_URL(url)\n",
    "\n",
    "    if bs == -1:\n",
    "        Write_Error(url,\"Error in opening the URL\",\"error.txt\")\n",
    "        #print(\"Error in opening the URL: {}\".format(url))\n",
    "        continue\n",
    "        \n",
    "\n",
    "    if bs == -2:\n",
    "        Write_Error(url,\"Error in the creation of bs object for the URL\",\"error.txt\")\n",
    "        #print(\"Error in the creation of bs object for the URL: {}\".format(url))\n",
    "        continue\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    #create a name for the files\n",
    "    #image_file_name = str(year)+\"_\"+movie.strip()+\".jpg\"\n",
    "    #text_file_name =  str(year)+\"_\"+movie.strip()+\".txt\"\n",
    "    image_file_name = str(Movie_ID)+\".jpg\"\n",
    "    text_file_name =  str(Movie_ID)+\".txt\"\n",
    "\n",
    "    text=Get_Plot(bs)\n",
    "    \n",
    "    if text == -1:\n",
    "        Write_Error(url,\"No paragraphs are found\",\"error.txt\")\n",
    "        #print(\"No paragraphs are found\")\n",
    "        #print(url)\n",
    "        continue\n",
    "\n",
    "    if text == -2:\n",
    "        Write_Error(url,\"Warning: No Plot ID found\",\"error.txt\")\n",
    "        #print(\"Warning: No Plot ID found\")\n",
    "        #print(url)\n",
    "        \n",
    "        text = Get_All_Text(bs)\n",
    "        if text == -1:\n",
    "            Write_Error(url,\"No paragraphs are found\",\"error.txt\")\n",
    "            #print(\"No paragraphs are found\")\n",
    "            #print(url)\n",
    "            continue\n",
    "        \n",
    "    if text == -3:\n",
    "        Write_Error(url,\"Error while appending the main plot with the initial paragraph\",\"error.txt\")\n",
    "        #print(\"Error while appending the main plot with the initial paragraph\")\n",
    "        #print(url)\n",
    "        continue\n",
    "    \n",
    "    status = Save_Text_File(text,text_file_name)\n",
    "    \n",
    "    if status == -1:\n",
    "        Write_Error(url,\"Not able to change the directory to ./data\",\"error.txt\")\n",
    "        #print(\"Not able to change the directory to ./data\")\n",
    "        #print(url)\n",
    "        continue\n",
    "    \n",
    "    if status == -2:\n",
    "        Write_Error(url,\"Not able to change the directory to .. (parent directory) from ./data\",\"error.txt\")\n",
    "        #print(\"Not able to change the directory to .. (parent directory) from ./data\")\n",
    "        #print(url)\n",
    "        continue\n",
    "        \n",
    "    #Downloading Image files    \n",
    "    status = Get_And_Save_Image(bs,image_file_name)\n",
    "    \n",
    "    if status == -1:\n",
    "        Write_Error(url,\"Not able to find the image\",\"error.txt\")\n",
    "        #print(\"Not able to find the image\")\n",
    "        #print(url)\n",
    "        continue\n",
    "\n",
    "    if status == -2:\n",
    "        Write_Error(url,\"Not able to save the image\",\"error.txt\")\n",
    "        #print(\"Not able to save the image\")\n",
    "        #print(url)\n",
    "        continue\n",
    "\n",
    "    #Check the status of the webbot    \n",
    "    tracker = tracker + 1\n",
    "    if (tracker % 100 == 0):\n",
    "        print(\"Processed {} URLs\".format(tracker))\n",
    "        end = time.time() # Get end time\n",
    "        elapsed_time = end - start\n",
    "        print(\"Elapsed time to process 100 URLs:{} secs\".format(elapsed_time))\n",
    "        start = time.time() # Get end time\n",
    "        #break\n",
    "\n",
    "    #if term == 1:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 4045 movies we obtained 4037 movies text successfully. But we were able to obtain only 3749 images, since images were not available to some of the movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Cleansing\n",
    "Now that we downloaded the data, let us clean the data to make the data ready for applying text analytics algorithms. For the purpose of data cleaning, we created the following 3 functions:\n",
    "\n",
    "* **Read_File(p)** It will open the input file, reads the text in the file, converts all the test to lower case, removes the punctuation (if any), and returns a list of tokens.\n",
    "\n",
    "* **Remove_Stop_Words(tokens)** It will remove all stop words from the list of input tokens. Returns a refined list of tokens, with no stop words\n",
    "\n",
    "\n",
    "* **Clean_Text(tokens)** It will clean all the text by removing any square brackets \"[...]\", braces \"(\" and \")\", commas, colons, apostrophes etc. Returns a list of tokens that are just alphanumeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Read_File(p):\n",
    "   with open(p, 'r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    #Convert all the text to lower case\n",
    "    #\n",
    "    lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    no_punctuation = lowers.translate(string.punctuation)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "def Remove_Stop_Words(tokens):\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "def Clean_Text(tokens):\n",
    "    text = \" \".join(tokens)\n",
    "    #Remove punctuation marks, text in [], (, ), :\n",
    "    filtered1 = re.sub('\\.|\\`|\\'|\\[.*\\]|\\(|\\)|,|:', \" \",text)\n",
    "    \n",
    "    #Remove any single characters\n",
    "    filtered1 = re.sub('(^| ).( |$)', \" \",filtered1)\n",
    "    #Remove any contiguous spaces    \n",
    "    filtered1 = re.sub(' +',\" \",filtered1)\n",
    "    \n",
    "    #Include only alpha numeric characters\n",
    "    filtered1=\" \".join([i for i in filtered1.split() if re.search('[0-9 a-z]*',i)])\n",
    "    return filtered1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block will use the above functions to clean the text. Do NOT run this code, unless you want to test it, since it will run for some time. To save time, we saved the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 files\n",
      "Processed 200 files\n",
      "Processed 300 files\n",
      "Processed 400 files\n",
      "Processed 500 files\n",
      "Processed 600 files\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "file_names = os.listdir(\"./data\")\n",
    "file_names = [i for i in file_names if re.search('[1-9]*\\.txt',i)]\n",
    "y = list()\n",
    "x = list()\n",
    "k = 0\n",
    "for i in file_names:\n",
    "    y.append(int(i.split(\".\")[0]))\n",
    "    #print(y)\n",
    "    tokens = Read_File(\"./data/\"+i)\n",
    "    tokens = Remove_Stop_Words(tokens)\n",
    "    cleaned_text = Clean_Text(tokens)\n",
    "    x.append(cleaned_text)\n",
    "    k = k+1\n",
    "    if(k%100 == 0):\n",
    "        print(\"Processed {} files\".format(k))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
